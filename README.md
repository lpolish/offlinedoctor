# Offline Doctor - AI Medical Assistant ğŸ¥

A cross-platform desktop application that provides AI-powered medical assistance without requiring an internet connection. Built with Electron and powered by Ollama for complete privacy and offline functionality.

![Offline Doctor](assets/screenshot.png)

## âœ¨ Features

- **ğŸ”’ Complete Privacy**: All data stays on your device
- **ğŸ“± Cross-Platform**: Works on Windows, macOS, and Linux
- **ğŸ¤– AI-Powered**: Uses advanced language models via Ollama
- **ğŸ’¬ Interactive Chat**: Natural conversation with medical AI
- **ğŸ” Symptom Checker**: Quick symptom analysis and guidance
- **ğŸ’Š Medication Tracker**: Track medications and check interactions
- **ğŸ“Š Medical History**: Local storage of consultation history
- **âš™ï¸ Configurable**: Adjust AI model settings and preferences

## ğŸš¨ Important Medical Disclaimer

**This application is for informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition.**

## ğŸ› ï¸ Prerequisites

Before installing, ensure you have:

- **Node.js** 16 or higher ([Download](https://nodejs.org/))
- **Python** 3.7 or higher ([Download](https://python.org/))
- **Git** (for cloning the repository)

## ğŸš€ Quick Start

### Automated Setup (Recommended)

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourorg/offline-doctor.git
   cd offline-doctor
   ```

2. **Run the setup script:**
   
   **Linux/macOS:**
   ```bash
   ./setup.sh
   ```
   
   **Windows:**
   ```cmd
   setup.bat
   ```

3. **Start the application:**
   ```bash
   npm start
   ```

### Manual Setup

1. **Clone and install dependencies:**
   ```bash
   git clone https://github.com/yourorg/offline-doctor.git
   cd offline-doctor
   npm install
   ```

2. **Set up Python backend:**
   ```bash
   cd backend
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   deactivate
   cd ..
   ```

3. **Install Ollama:**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Windows: Download from https://ollama.ai/download
   ```

4. **Pull the medical AI model:**
   ```bash
   ollama pull llama2
   ```

5. **Start the application:**
   ```bash
   npm start
   ```

## ğŸ“¦ Building Distributables

Create installer packages for different platforms:

```bash
# Build for current platform
npm run build

# Platform-specific builds
npm run build-win     # Windows NSIS installer
npm run build-mac     # macOS DMG
npm run build-linux   # Linux AppImage and DEB
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend        â”‚    â”‚   AI Engine     â”‚
â”‚   (Electron)    â”‚â—„â”€â”€â–ºâ”‚   (Python Flask) â”‚â—„â”€â”€â–ºâ”‚   (Ollama)      â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ HTML/CSS/JS   â”‚    â”‚ â€¢ REST API       â”‚    â”‚ â€¢ Llama2 Model  â”‚
â”‚ â€¢ Chat UI       â”‚    â”‚ â€¢ Medical Logic  â”‚    â”‚ â€¢ Local Inferenceâ”‚
â”‚ â€¢ Settings      â”‚    â”‚ â€¢ Data Storage   â”‚    â”‚ â€¢ No Internet   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- **Frontend (Electron)**: Cross-platform desktop UI with medical consultation interface
- **Backend (Python Flask)**: API server handling medical queries and business logic
- **AI Engine (Ollama)**: Local language model serving medical AI responses
- **Data Storage**: Local SQLite/JSON for medical history and user preferences

## ğŸ”§ Configuration

### AI Model Settings

The application supports multiple AI models:

- **llama2** (Recommended): Best balance of performance and accuracy
- **mistral**: Faster responses, good for basic queries
- **codellama**: Specialized for technical medical information

Configure in the Settings tab or modify `backend/server.py`:

```python
DEFAULT_MODEL = "llama2"  # Change to your preferred model
```

### Privacy Settings

- **Save History**: Toggle consultation history storage
- **Anonymize Data**: Remove personal identifiers from stored data
- **Auto-clear**: Automatically clear history after specified time

## ğŸ—‚ï¸ Project Structure

```
offline-doctor/
â”œâ”€â”€ main.js              # Main Electron process
â”œâ”€â”€ preload.js           # Security context bridge
â”œâ”€â”€ renderer.js          # Frontend application logic
â”œâ”€â”€ index.html           # Main UI layout
â”œâ”€â”€ style.css            # Application styles
â”œâ”€â”€ package.json         # Node.js dependencies and scripts
â”œâ”€â”€ setup.sh            # Linux/macOS setup script
â”œâ”€â”€ setup.bat           # Windows setup script
â”œâ”€â”€ assets/             # Icons and images
â”œâ”€â”€ backend/            # Python Flask server
â”‚   â”œâ”€â”€ server.py       # Main API server
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ venv/           # Python virtual environment
â””â”€â”€ .github/
    â””â”€â”€ copilot-instructions.md
```

## ğŸ”’ Security & Privacy

- **No External Connections**: All processing happens locally
- **Data Encryption**: Sensitive data encrypted at rest
- **Memory Safety**: Secure cleanup of medical data in memory
- **Access Control**: No unauthorized access to medical history
- **HIPAA Considerations**: Designed with healthcare privacy in mind

## ğŸ§ª Development

### Running in Development Mode

```bash
npm run dev
```

### Backend Development

```bash
cd backend
source venv/bin/activate
python server.py
```

### Adding New Features

1. **Frontend Features**: Modify `renderer.js` and update UI in `index.html`
2. **Backend API**: Add endpoints in `backend/server.py`
3. **AI Prompts**: Update medical context in `MedicalAI` class

## ğŸ› Troubleshooting

### Common Issues

**Ollama not starting:**
```bash
# Check if Ollama is running
ps aux | grep ollama

# Manually start Ollama
ollama serve
```

**Python virtual environment issues:**
```bash
# Recreate virtual environment
rm -rf backend/venv
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**Model download failures:**
```bash
# Check available models
ollama list

# Re-download model
ollama pull llama2
```

### Log Files

- **Electron logs**: Check console in DevTools (F12)
- **Backend logs**: Terminal output when running `python server.py`
- **Ollama logs**: Check `~/.ollama/logs/` directory

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with detailed description

### Medical Content Guidelines

- Always include appropriate disclaimers
- Cite medical sources when possible
- Avoid providing specific dosage recommendations
- Emphasize the importance of professional medical care

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local AI model hosting
- [Electron](https://electronjs.org/) for cross-platform desktop framework
- [Flask](https://flask.palletsprojects.com/) for Python web framework
- Medical AI community for guidance on responsible AI healthcare applications

## ğŸ“ Support

For support and questions:

- **Issues**: [GitHub Issues](https://github.com/yourorg/offline-doctor/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourorg/offline-doctor/discussions)
- **Email**: support@offlinedoctor.com

---

**Remember**: This tool is designed to supplement, not replace, professional medical advice. Always consult healthcare professionals for serious medical concerns.
